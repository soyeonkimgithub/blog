---
layout: post
title: Bias-Variance Tradeoff
categories: machinelearning
sitemap: false
hide_last_modified: true
published: true
---
## [ML] AUC ROC

### background

One important aspect of Machine Learning is model evaluation. This is where these performance metrics come into the picture, they give us a sense of how good a model is. 

### description

- AUC : Area Under Curve
- AUC curve represents the area under the ROC curve. It measures the overall performance of the binary classification model. Our goal is to maximise this area in order to have the highest TPR, and lowest FPR at the given threshold. The AUC measures the probability that the model will assign a randomly chosen positive instance a higher predicted probability compared to a randomly chosen negative instance.
- ROC : Receiver Operating Characteristics
- ROC curve is the graphical representation of the effectiveness of the binary classification model. It plots true positive rate (TPR) vs the false positive rate (FPR) at different classification thresholds.
- TPR is the proportion of observations that were correctly predicted to be positive
- FPR is the proportion of observation that were incorrectly predicted to be positive

### how to use
~~~python
from sklearn.metrics import roc_auc_score
auc = np.round(roc_auc_score(y_true, y_pred), 3)
print("Auc for our sample data is {}".format(auc))
~~~