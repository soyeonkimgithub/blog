---
layout: post
title: Auto Word Association
image: 
  path: /assets/img/post/project/AWA-WordCloud.png
description: >
  This is school capstone project, worked with other 4 students.
category: project
hide_last_modified: true
published: true
---
## [School]Auto Word Association

### Summary
* Task: Deliver a similarity matrix of medical terms
(This project aims to tokenize words and phrases and measure the relatedness degree of themselves through unsupervised learning from the given text corpus.)
* Background : With the rapid development of medical information and research, the number of medical literatures also showed explosive growth. However, the different expressions of medical knowledge lead to the redundancy and heterogeneity, and make it difficult to achieve a unified and standardised expression of medical vocabularies. This undoubtedly adds a lot of burden to medical professionals in real-life, especially when reading and screening literatures, that target literatures are often missed due to lack of synonyms or related words. 
* Data
- Corpus consisting of research article titles and abstracts from medical domain sources, provided from client
- 'PubMed 2018 Baseline': PubMed is a search engine accessing primarily the MEDLINE database of references and abstracts on life sciences and biomedical topics. 
- Human evaluated term-pair dataset(Pedersen, Hliaoutakis, MayoSRS, UMNSRS, Bio-SimVerb, Bio-SimLex): test model on biomedical measurement dataset that are commonly used for measuring semantic similarity between medical terms
* Model/Algorithms: Word2vec(CBOW, skip-gram), FastText, GloVe
* Evaluation metrics: cosine similarity 

### Key Points
1. Retrieved 250 million terms
2. Trained word embedding models with different training size, and number of iterations
3. Evaluated models against human evaluated medical term pairs (cosine similarity)
4. Compared client system(EzyReviewer) synonym sets and our models' synonym sets
5. Compared two results by medical experts
6. FastText outperforms Word2vec on some of the evaluation sets when corpus size is small â†’ This may be since FastText learns representations of words in terms of sub-word n-grams. When the number of iterations is small, FastText can learn n-grams across words to create better representations of rare words than Word2vec. This advantage disappears as iterations increase and Word2vec learns reasonable embeddings without sub-word information.



{:.text-align-center}
![400x200](/assets/img/post/project/AWA-FrequentWord.png){:width="45%"}
![400x200](/assets/img/post/project/AWA-PCA.png){:width="45%"}
test
![400x200](/assets/img/post/project/AWA-tSNE.png){:width="45%"}
![400x200](/assets/img/post/project/AWA-CompareModels.png){:width="45%"}
test
![400x200](/assets/img/post/project/AWA-SimilarityWords.png){:width="45%"}
![400x200](/assets/img/post/project/AWA-SimilarityMatrix.png){:width="45%"}
test
![400x200](/assets/img/post/project/AWA-Survey.png){:width="45%"}

### Results
Word2Vec skip-gram model with 200 dimensions was selected, and 75% of survey options, a majority of respondents selected the synonym set generated by our system. 


### Download
* <a href="https://github.com/soyeonkimgithub/Volatility-Forecasting/blob/main/CS5703_Presentation.pdf">presentation ppt</a>
* <a href="https://github.com/soyeonkimgithub/AWA/blob/main/AutoWordAssociation_Core.ipynb">code</a>

### Code
<iframe src="https://nbviewer.org/gist/soyeonkimgithub/3fcf7d5796d1fbe122a92c5b280408a2" width="1000" height="1500" scrolling="yes" frameborder="1"></iframe>

