---
layout: post
title: Natural Language Processing
categories: study
sitemap: false
hide_last_modified: true
published: true

---
## [ML] Natural Language Processing

### Natural Language Processing

- a document represented as a vector of word counts is called a ‘bag of words’
- we can use cosine similarity on the vectors made to determine similarity
- we can improve on bag of words by adjusting word counts based on their frequency in corpus (the group of all the documents)
- we can use TF-IDF(Term Frequency - Inverse Document Frequency)
- Term Frequency: importance of the term within that document
    - TF(d,t) = number of occurrences of term t in document d
- Inverse Document Frequency: importance of the term in the corpus
    - IDF(t) = log(D/t) where
    - D = total number of documents
    - t = number of documents with the term
- Stemming : text preprocessing technique to reduces words to their root form

~~~python
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.feature_extraction.text import TfidfTransformer
from sklearn.pipeline import Pipeline
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, classification_report

vect = CountVectorizer()
X = vect.fit_transform(X)
X_train, X_test, y_train, y_test =  train_test_split(X, y, test_size=0.3, random_state=101)

nb = MultinomialNB()
nb_model = nb.fit(X_train, y_train)
pred = nb_model.predict(X_test)

print(confusion_matrix(y_test, pred))
print(classification_report(y_test, pred))

pipeline = Pipeline([
    ('bow', CountVectorizer()),
    ('tfidf', TfidfTransformer()), 
    ('classifier', MultinomialNB())
])
X = yelp_class['text']
y = yelp_class['stars']
X_train, X_test, y_train, y_test =  train_test_split(X, y, test_size=0.3, random_state=101)

pipeline.fit(X_train, y_train)
predictions = pipeline.predict(X_test)

print(confusion_matrix(y_test, predictions))
print(classification_report(y_test, predictions))
~~~