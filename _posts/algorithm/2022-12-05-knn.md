---
layout: post
title: K Neareste Neighbours
categories: study
sitemap: false
hide_last_modified: true
published: true
---
## [Algorithm] K Nearest Neighbours

### KNN
* classification algorithm
* training algorithm : store all the data
* prediction algorithm :
1. calculate the distance from x which indicates particular new data, to all points in your data
2. sort the points in your data by increasing distance from x
3. predict the majority label of the ‘k’ closest points
* Choosing a ‘K’ will affect what class a new point is assigned to → how many points are you going to look at that are next to your new test point
* Pros : very simple, training is trivial, works with any number of classes, easy to add more data, few parameters (k and distance metric)
* Cons : high prediction cost (worse for large data sets) → because you’re having to sort this entire large dataset, not good with high dimensional data, categorical features don’t work well.


~~~python
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import confusion_matrix, classification_report

# standardise the variables
scaler = StandardScaler()
scaler.fit(df.drop('TARGET CLASS', axis=1))
scaled_features = scaler.transform(df.drop('TARGET CLASS', axis=1))
df_feat = pd.DataFrame(scaled_features, columns=df.columns[:-1])

# train, predict
knn = KNeighborsClassifier(n_neighbors=1)
knn.fit(X_train, y_train)
pred = knn.predict(X_test)
print(confusion_matrix(y_test, pred))
print(classification_report(y_test, pred))

# choose k
error_rate = []

for i in range(1,40):
    knn = KNeighborsClassifier(n_neighbors=i)
    knn.fit(X_train, y_train)
    pred_i = knn.predict(X_test)
    error_rate.append(np.mean(pred_i != y_test))

plt.figure(figsize=(10, 6))
plt.plot(range(1, 40), error_rate, linestyle='dashed', markerfacecolor='red', markersize=10, marker='o')
plt.title('Error Rate vs. K Value')
plt.xlabel('K')
plt.ylabel('Error Rate')    
~~~
